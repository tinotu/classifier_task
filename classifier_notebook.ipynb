{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning) # for more readable code\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have been given a dataset without context with the sole goal to create a model to predict the boolean target variable. Let's have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>numeric0</th>\n",
       "      <th>numeric1</th>\n",
       "      <th>categorical0</th>\n",
       "      <th>time</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-11-24</td>\n",
       "      <td>2515</td>\n",
       "      <td>2.0</td>\n",
       "      <td>c</td>\n",
       "      <td>01:33:52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-03-05</td>\n",
       "      <td>5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>20:09:27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-23</td>\n",
       "      <td>5930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>11:22:35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992-02-16</td>\n",
       "      <td>2967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>19:18:36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971-06-06</td>\n",
       "      <td>194</td>\n",
       "      <td>5.0</td>\n",
       "      <td>c</td>\n",
       "      <td>20:01:58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  numeric0  numeric1 categorical0      time  target\n",
       "0 2015-11-24      2515       2.0            c  01:33:52       0\n",
       "1 2009-03-05      5156       NaN            b  20:09:27       0\n",
       "2 2015-12-23      5930       NaN            b  11:22:35       0\n",
       "3 1992-02-16      2967       NaN            b  19:18:36       0\n",
       "4 1971-06-06       194       5.0            c  20:01:58       0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data_science_assignment_data.csv', parse_dates=['date'])\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          10000 non-null  datetime64[ns]\n",
      " 1   numeric0      10000 non-null  int64         \n",
      " 2   numeric1      5083 non-null   float64       \n",
      " 3   categorical0  10000 non-null  object        \n",
      " 4   time          10000 non-null  object        \n",
      " 5   target        10000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have a date, two numeric, a categorical and a time variable. The target variable is boolean. We are missing around half of the observations for the numeric1 variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I create new features from the date and time features to be able to explore their distribution and correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data.copy()\n",
    "clean_data['year'] = clean_data['date'].dt.year\n",
    "clean_data['month'] = clean_data['date'].dt.month\n",
    "clean_data['day'] = clean_data['date'].dt.day\n",
    "clean_data['hour'] = [int(x[:2]) for x in clean_data.time]\n",
    "clean_data['minute'] = [int(x[3:5]) for x in clean_data.time]\n",
    "clean_data['second'] = [int(x[6:]) for x in clean_data.time]\n",
    "clean_data['weekday'] = clean_data['date'].dt.weekday\n",
    "\n",
    "clean_data = clean_data.drop(['date', 'time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a sample of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric0</th>\n",
       "      <th>numeric1</th>\n",
       "      <th>categorical0</th>\n",
       "      <th>target</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "      <td>2.0</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194</td>\n",
       "      <td>5.0</td>\n",
       "      <td>c</td>\n",
       "      <td>0</td>\n",
       "      <td>1971</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numeric0  numeric1 categorical0  target  year  month  day  hour  minute  \\\n",
       "0      2515       2.0            c       0  2015     11   24     1      33   \n",
       "1      5156       NaN            b       0  2009      3    5    20       9   \n",
       "2      5930       NaN            b       0  2015     12   23    11      22   \n",
       "3      2967       NaN            b       0  1992      2   16    19      18   \n",
       "4       194       5.0            c       0  1971      6    6    20       1   \n",
       "\n",
       "   second  weekday  \n",
       "0      52        1  \n",
       "1      27        3  \n",
       "2      35        2  \n",
       "3      36        6  \n",
       "4      58        6  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each time unit is now represented as its own feature and \"weekday\" is presented as new information. Next, I proceed to analyse the statistics and distributions of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric0</th>\n",
       "      <th>numeric1</th>\n",
       "      <th>target</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>5083.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4981.998600</td>\n",
       "      <td>4.540035</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>1994.251800</td>\n",
       "      <td>6.479500</td>\n",
       "      <td>15.787300</td>\n",
       "      <td>11.302900</td>\n",
       "      <td>29.489700</td>\n",
       "      <td>29.671400</td>\n",
       "      <td>2.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.313858</td>\n",
       "      <td>2.863159</td>\n",
       "      <td>0.339592</td>\n",
       "      <td>14.213022</td>\n",
       "      <td>3.408217</td>\n",
       "      <td>8.783995</td>\n",
       "      <td>6.944017</td>\n",
       "      <td>17.219813</td>\n",
       "      <td>17.301756</td>\n",
       "      <td>2.010009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1970.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2499.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1982.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4940.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7487.250000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9999.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2019.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           numeric0     numeric1        target          year         month  \\\n",
       "count  10000.000000  5083.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean    4981.998600     4.540035      0.133000   1994.251800      6.479500   \n",
       "std     2886.313858     2.863159      0.339592     14.213022      3.408217   \n",
       "min        0.000000     0.000000      0.000000   1970.000000      1.000000   \n",
       "25%     2499.750000     2.000000      0.000000   1982.000000      4.000000   \n",
       "50%     4940.500000     5.000000      0.000000   1994.000000      6.000000   \n",
       "75%     7487.250000     7.000000      0.000000   2006.000000      9.000000   \n",
       "max     9999.000000     9.000000      1.000000   2019.000000     12.000000   \n",
       "\n",
       "                day          hour        minute        second       weekday  \n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000  \n",
       "mean      15.787300     11.302900     29.489700     29.671400      2.983600  \n",
       "std        8.783995      6.944017     17.219813     17.301756      2.010009  \n",
       "min        1.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%        8.000000      5.000000     15.000000     15.000000      1.000000  \n",
       "50%       16.000000     11.000000     29.000000     30.000000      3.000000  \n",
       "75%       23.000000     17.000000     44.000000     45.000000      5.000000  \n",
       "max       31.000000     23.000000     59.000000     59.000000      6.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.hist(layout=(4,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,3)\n",
    "clean_data.groupby(data['categorical0'])['categorical0'].count().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems most of our features are in uniform distribution. The distribution of the target variable is unbalanced which has to be taken into account when training and evaluating the models. Let's see how much of target class are positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.133 of the observations are from positive target class\n"
     ]
    }
   ],
   "source": [
    "print(f'{clean_data.target.sum()/clean_data.target.shape[0]} of the observations are from positive target class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I one-hot-encode the categorical0 feature, so I'm only dealing with numerical data and able to compute correlations across the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.get_dummies(clean_data, prefix=\"cat0\", columns=['categorical0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the pearson correlation coefficients between the features are. Correlation with the target variable suggests that feature can be useful as a predictor at least with linear models. High correlation with other features suggests feature collinearity, which we would like to eliminate before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "sns.heatmap(clean_data.corr(), vmin=-1, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn some things from the correlations. Firstly, the categorical0 and numeric0 features have some correlation with the target. There doesn't seem to be a lot of information we can extract from the others. We also learn that none of the variables correlate with numeric1, which is an indication not to try to predict the missing values from the other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try if we get lucky, and the \"Nan\" value of numeric1 is meaningful information by engineering a new feature \"numeric1_is_nan\" with a boolean value. Because our algorithms cannot handle missing data and better solutions have not emerged, let's impute the missing values with the feature's mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['numeric1_is_nan'] = [1 if math.isnan(x) == True else 0 for x in clean_data['numeric1']]\n",
    "numeric1_mean = clean_data['numeric1'].mean()\n",
    "clean_data['numeric1']  = [numeric1_mean if math.isnan(x) == True else x for x in clean_data['numeric1']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the correlations again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "sns.heatmap(clean_data.corr(), vmin=-1, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No luck there - imputing didn't improve the correlation, and the new feature numeric1_is_nan doesn't show correlation with the target feature.\n",
    "\n",
    "Moving on, the categorical0 has some correlation I want to dig deeper into. Let's have a look how the positive values are distributed along this feature by computing the number of ones in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categorical0\n",
       "a    1124\n",
       "b      97\n",
       "c     109\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('categorical0')['target'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamma mia! Our minority class is heavily correlated to the category \"a\". This is a pattern a classifier will certainly learn, but we need to make sure it is not the only factor in the decision. This takes to us to the end of the exploratory analysis phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation coefficients are more important in feature selection for linear models, but more complex non-linear models can learn patterns from combinations of several features. \n",
    "\n",
    "#### At this point, I have to select a classifier algorithm for the task. I will start with Gradient Boosted Decision Trees -algorithm from scikit-learn library for in my experience it has been a very versatile classifier for different tasks.\n",
    "\n",
    "I will next conduct feature selection by training a Gradient Boosted Decision Trees -algorithm and computing its feature importance -metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to normalize the data so the different scales of the features don't throw off the classifier algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric0</th>\n",
       "      <th>numeric1</th>\n",
       "      <th>target</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>weekday</th>\n",
       "      <th>cat0_a</th>\n",
       "      <th>cat0_b</th>\n",
       "      <th>cat0_c</th>\n",
       "      <th>numeric1_is_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.498250</td>\n",
       "      <td>0.504448</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.494935</td>\n",
       "      <td>0.498136</td>\n",
       "      <td>0.492910</td>\n",
       "      <td>0.491430</td>\n",
       "      <td>0.499825</td>\n",
       "      <td>0.502905</td>\n",
       "      <td>0.497267</td>\n",
       "      <td>0.332400</td>\n",
       "      <td>0.334100</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288660</td>\n",
       "      <td>0.226799</td>\n",
       "      <td>0.339592</td>\n",
       "      <td>0.290062</td>\n",
       "      <td>0.309838</td>\n",
       "      <td>0.292800</td>\n",
       "      <td>0.301914</td>\n",
       "      <td>0.291861</td>\n",
       "      <td>0.293250</td>\n",
       "      <td>0.335001</td>\n",
       "      <td>0.471097</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.471487</td>\n",
       "      <td>0.499956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.494099</td>\n",
       "      <td>0.504448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.748800</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           numeric0      numeric1        target          year         month  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.498250      0.504448      0.133000      0.494935      0.498136   \n",
       "std        0.288660      0.226799      0.339592      0.290062      0.309838   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.250000      0.444444      0.000000      0.244898      0.272727   \n",
       "50%        0.494099      0.504448      0.000000      0.489796      0.454545   \n",
       "75%        0.748800      0.555556      0.000000      0.734694      0.727273   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                day          hour        minute        second       weekday  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.492910      0.491430      0.499825      0.502905      0.497267   \n",
       "std        0.292800      0.301914      0.291861      0.293250      0.335001   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.233333      0.217391      0.254237      0.254237      0.166667   \n",
       "50%        0.500000      0.478261      0.491525      0.508475      0.500000   \n",
       "75%        0.733333      0.739130      0.745763      0.762712      0.833333   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             cat0_a        cat0_b        cat0_c  numeric1_is_nan  \n",
       "count  10000.000000  10000.000000  10000.000000     10000.000000  \n",
       "mean       0.332400      0.334100      0.333500         0.491700  \n",
       "std        0.471097      0.471698      0.471487         0.499956  \n",
       "min        0.000000      0.000000      0.000000         0.000000  \n",
       "25%        0.000000      0.000000      0.000000         0.000000  \n",
       "50%        0.000000      0.000000      0.000000         0.000000  \n",
       "75%        1.000000      1.000000      1.000000         1.000000  \n",
       "max        1.000000      1.000000      1.000000         1.000000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "feature_columns = [x for x in clean_data.columns if x != \"target\" ]\n",
    "clean_data[feature_columns] = scaler.fit_transform(clean_data[feature_columns])\n",
    "clean_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the feature is now scaled down to range from 0 to 1 instead of 0 and 100000.\n",
    "\n",
    "The target feature is heavily unbalanced in our dataset. I will address this by limiting the number of observations for the majority class in the training set by creating a new sample including all the positive samples and randomly picking negative samples so that the dataset used in modeling has positive and negative samples with the ratio of 1:2 (ratio found through experimenting with different ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_samples = clean_data[clean_data.target == 1]\n",
    "negative_sample_multiplier = 2\n",
    "picked_negative_samples = clean_data[clean_data.target == 0].sample(all_positive_samples.shape[0] * negative_sample_multiplier, random_state=123)\n",
    "balanced_dataset = pd.concat([all_positive_samples, picked_negative_samples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, I will create the train and test datasets from the balanced dataset so that the ratio of positive and negative classes is equal in both samples so that the hold-out test set is comparable to the distribution of the original dataset. I will hold-out 0.2 of the balanced data set for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = balanced_dataset[['target']]\n",
    "X = balanced_dataset.drop(columns='target')\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=1337)\n",
    "for train, test in sss.split(X,y): \n",
    "    X_train= X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to compare my models efficiently, I define a function that fits a model on the training data and prints the cross-validated Balanced Accuracy Score metric with the feature importance graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cv_model(algorithm, X_train, y_train, performCV=True, printFeatureImportance=True, cv_folds=3):\n",
    "    #Fit the algorithm on the data\n",
    "    features = X_train.columns\n",
    "    algorithm.fit(X_train, y_train)\n",
    "        \n",
    "    #Predict training set:\n",
    "    train_predictions = algorithm.predict(X_train)\n",
    "    \n",
    "    #Perform cross-validation:\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(algorithm, X_train, y_train, cv=cv_folds, scoring='balanced_accuracy')\n",
    "    \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy (Train): %.4g\" % metrics.accuracy_score(y_train.values, train_predictions))\n",
    "    print(\"Balanced Accuracy score(Train): %f\" % metrics.balanced_accuracy_score(y_train, train_predictions))\n",
    "    \n",
    "    if performCV:\n",
    "        print(f'Mean Balanced Accuracy score (CV) : {np.mean(cv_score)} | Std - {np.std(cv_score)}')\n",
    "        \n",
    "    #Print Feature Importance:\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(algorithm.feature_importances_, features).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first train a baseline GradientBoostingClassifier model for feature selection with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train): 0.8618\n",
      "Balanced Accuracy score(Train): 0.867011\n",
      "Mean Balanced Accuracy score (CV) : 0.7467064582731715 | Std - 0.006468378957218444\n"
     ]
    }
   ],
   "source": [
    "gbc_baseline = GradientBoostingClassifier(random_state=1337)\n",
    "fit_cv_model(gbc_baseline, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance metric tells how much the features influenced the classification decision on this particular model and is not universal truth for the data. Let's see if a model trained with same parameters and just the two \"important\" features \"cat0_a\" and \"numeric0\" does any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train): 0.8261\n",
      "Balanced Accuracy score(Train): 0.846100\n",
      "Mean Balanced Accuracy score (CV) : 0.7692489871589642 | Std - 0.009216555646709632\n"
     ]
    }
   ],
   "source": [
    "gbc_baseline_selected_features = GradientBoostingClassifier(random_state=1337)\n",
    "fit_cv_model(gbc_baseline, X_train[['cat0_a', 'numeric0']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Balanced Accuracy score (CV) went up from 0.747 to 0.769 which indicates that we are able to make more robust model by removing the unimportant features. Let's just leave cat0_a and numeric0 as the predictor features in our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['cat0_a', 'numeric0']]\n",
    "X_test = X_test[['cat0_a', 'numeric0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are down to two predictor features. cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will next perform hyper parameter optimization for the model.\n",
    "\n",
    "Optimization is performed by grid search method:\n",
    "1. Train and cross-validate baseline model with starting parameters.\n",
    "2. Cross-validate a bunch of models with different parameter values for parameters \"A\" and \"B\".\n",
    "3. Take parameter values for \"A\" and \"B\" from the best performing model and use them in the next round of testing with new parameters \"C\" and \"D\"\n",
    "4. Repeat step 3 until all wanted parameters have been optimized.\n",
    "5. Check how validation score has developed compared to baseline.\n",
    "\n",
    "I will optimize:\n",
    "    'n_estimators',\n",
    "    'learning_rate',\n",
    "    'max_depth',\n",
    "    'min_samples_split',\n",
    "    'min_samples_leaf',\n",
    "    'max_features'\n",
    "parameters for balanced accuracy score metric.\n",
    "\n",
    "Our baseline was model gbc_baseline_selected_features with balanced accuracy score of 0.769\n",
    "\n",
    "#### The next cells are commented out because the optimization takes several minutes and who has several minutes these days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 1:  n_estimators and learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test1 = {'n_estimators':[500,1000,1500], 'learning_rate':[0.1, 0.05, 0.01]}\n",
    "# gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(random_state=1337), \n",
    "# param_grid = param_test1, scoring='balanced_accuracy',n_jobs=4, cv=3)\n",
    "# gsearch1.fit(X_train, y_train)\n",
    "# print(gsearch1.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 2: max_depth and min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test2 = {'max_depth':range(3,10,2), 'min_samples_split':range(100,500,100)}\n",
    "# gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(**gsearch1.best_params_, random_state=1337),\n",
    "# param_grid = param_test2, scoring='balanced_accuracy',n_jobs=4, cv=3)\n",
    "# gsearch2.fit(X_train, y_train);\n",
    "# print(gsearch2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 3: min_samples_leaf and max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test3 = {'min_samples_leaf':range(20,61,10), 'max_features':['auto', 'sqrt', 'log2']}\n",
    "# gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(**gsearch1.best_params_, **gsearch2.best_params_, random_state=1337),\n",
    "# param_grid = param_test3, scoring='balanced_accuracy',n_jobs=4, cv=3)\n",
    "# gsearch3.fit(X_train, y_train);\n",
    "# print(gsearch3.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round 4: learning parameters n_estimators and learning_rate again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test4 = {'n_estimators':range(500,1500,100), 'learning_rate':[0.1, 0.05, 0.01]}\n",
    "# gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(**gsearch2.best_params_, **gsearch3.best_params_, random_state=1337),\n",
    "# param_grid = param_test4, scoring='balanced_accuracy',n_jobs=4, cv=3)\n",
    "# gsearch4.fit(X_train, y_train);\n",
    "# print(gsearch4.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'n_estimators': 500,\n",
       " 'max_depth': 3,\n",
       " 'min_samples_split': 200,\n",
       " 'min_samples_leaf': 40,\n",
       " 'max_features': 'sqrt'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_params = {'learning_rate': 0.01, 'n_estimators': 500, 'max_depth': 3, 'min_samples_split': 200, 'min_samples_leaf': 40, 'max_features':'sqrt'}\n",
    "optimized_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the best parameters found though grid search optimization. Let's see how our optimized model performs with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy (Train): 0.8004\n",
      "Balanced Accuracy score(Train): 0.823073\n",
      "Mean Balanced Accuracy score (CV) : 0.7777033408406261 | Std - 0.006970313457096546\n"
     ]
    }
   ],
   "source": [
    "gbc1 = GradientBoostingClassifier(**optimized_params)\n",
    "fit_cv_model(gbc1, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training accuracy was 0.800 and cross-validated balanced accuracy score went up from 0.769 to 0.785. Parameter optimization has improved our model, at least with the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In model evaluation I am using the Balanced Accuracy Score metric, which was also used in validating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our optimized model gbc1 performs on the test data. First I define a function to train and evaluate models with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    tn = cm[0][0]\n",
    "    fn = cm[1][0]\n",
    "    tp = cm[1][1]\n",
    "    fp = cm[0][1]\n",
    "    \n",
    "    print(f'Balanced accuracy score {metrics.balanced_accuracy_score(y_test, predictions)}')\n",
    "    print(f'True positive rate is: {tp/(tp+fn)}')\n",
    "    print(f'True negative rate is: {tn/(tn+fp)}')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of model gbc1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score 0.7922932330827068\n",
      "True positive rate is: 0.8383458646616542\n",
      "True negative rate is: 0.7462406015037594\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(gbc1, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model performs even better with the test set with balanced accuracy score of 0.792. This which means our model is not over fit.\n",
    "\n",
    "Our model got 83% of the positive label and 75% of the negative class correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment on different class ratios in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment by training Gradient Boosted Classifiers with same parameters but training with (1:1) or more (3:1) ratio of negative to positive samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model gbc2 - less negative samples (1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score 0.8421052631578947\n",
      "True positive rate is: 0.9887218045112782\n",
      "True negative rate is: 0.6954887218045113\n"
     ]
    }
   ],
   "source": [
    "# code to resample data\n",
    "all_positive_samples = clean_data[clean_data.target == 1]\n",
    "negative_sample_multiplier = 1\n",
    "picked_negative_samples = clean_data[clean_data.target == 0].sample(all_positive_samples.shape[0] * negative_sample_multiplier, random_state=1)\n",
    "balanced_dataset = pd.concat([all_positive_samples, picked_negative_samples], ignore_index=True)\n",
    "\n",
    "y = balanced_dataset[['target']]\n",
    "X = balanced_dataset[['cat0_a', 'numeric0']]\n",
    "\n",
    "# code to split the resampled data\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=1337)\n",
    "for train, test in sss.split(X,y): \n",
    "    X_train= X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "\n",
    "# code to train a model with same parameters and new training data \n",
    "gbc2 = GradientBoostingClassifier(**optimized_params)\n",
    "train_and_evaluate(gbc2, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By further limiting the number of negative samples in the training data, we can get a model that predicts almost all (99%) positive labels correctly with the setback that only 70% of the negative label are correct. This might be a more favorable outcome, depending on the context of the task.\n",
    "\n",
    "Let's try if adding negative samples has the opposite effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model gbc3 - more negative samples (3:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score 0.6284461152882206\n",
      "True positive rate is: 0.2819548872180451\n",
      "True negative rate is: 0.974937343358396\n"
     ]
    }
   ],
   "source": [
    "# code to resample data\n",
    "all_positive_samples = clean_data[clean_data.target == 1]\n",
    "negative_sample_multiplier = 3\n",
    "picked_negative_samples = clean_data[clean_data.target == 0].sample(all_positive_samples.shape[0] * negative_sample_multiplier, random_state=1)\n",
    "balanced_dataset = pd.concat([all_positive_samples, picked_negative_samples], ignore_index=True)\n",
    "\n",
    "y = balanced_dataset[['target']]\n",
    "X = balanced_dataset[['cat0_a', 'numeric0']]\n",
    "\n",
    "# code to split the re-sampled data\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=1337)\n",
    "for train, test in sss.split(X,y): \n",
    "    X_train= X.iloc[train]\n",
    "    y_train = y.iloc[train]\n",
    "    X_test = X.iloc[test]\n",
    "    y_test = y.iloc[test]\n",
    "\n",
    "# code to train a model with same parameters and new training data \n",
    "gbc3 = GradientBoostingClassifier(**optimized_params)\n",
    "train_and_evaluate(gbc3, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our overall balanced accuracy dropped to 0.627 as we get almost all (97%) of the negative label correctly, but only 28% of the positive label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without a clear goal, it is difficult to say which one is the \"best\" performing model, but based on balanced accuracy score it would be model gbc2 with score of 0.842. This model was trained with data that had 1:1 ratio of positive and negative samples. The scores aren't actually fully comparable as they are evaluated with different datasets. The point of this experiment was to show that different outcomes can be reached when the classification objective and context is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task was to train a binary classifier on a data without context and without a set goal metric. I performed:\n",
    "1. exploratory analysis on the data to find the distributions and correlations of the features\n",
    "2. some feature engineering to make more use of the data - unsuccessfully\n",
    "3. feature selection based on pearson correlation efficients and Gradient Boosted Classifier feature importance metrics\n",
    "4. parameter optimization with grid search and cross validation\n",
    "5. model evaluation on balanced accuracy score metric and by analyzing the confluence metric\n",
    "6. experimented on different ratios of minority and majority classes present in the training data\n",
    "\n",
    "I was able to get the balanced accuracy score up to 0.842 and found that I can optimize the accuracy for either of the classes by tuning the amount of negative class present in the training data, but and compromising on the accuracy on the other class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-cfb5db56",
   "language": "python",
   "display_name": "PyCharm (classifier_task)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}